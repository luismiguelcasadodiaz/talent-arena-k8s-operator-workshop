## Part One

You should have your k8s cluster running locally ready to install the operator we will prepare in 
this workshop. If you don't have it yet, follow the steps in [this readme](../README.md).

In this first part we will create learn how to deploy our operator in a k8s cluster, how to define 
a custom resource definition, and how the operator can react to resource changes. The following resources can be
useful to learn more about the components we are going to build in this first part: 

* [The operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/)
* [Custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
* [Kubebuilder introduction](https://book.kubebuilder.io/introduction)

This first part will help you setting up a K8S operator that is able to react to changes of our own
CRD.

## Custom Resource Definition (CRD)

This is the definition of a CRD from the K8S documentation:

> A custom resource is an extension of the Kubernetes API that is not necessarily available in a default Kubernetes installation.
> It represents a customization of a particular Kubernetes installation.
> However, many core Kubernetes functions are now built using custom resources, making Kubernetes more modular.

In this first part we will create the CRD that you will use in part two to deploy new LLM models
based on a prompt that the new model should answer successfully. When a CRD is defined, we need to 
define a spec to let the K8S controller know which is the desired state. The controller will then 
react on that spec and make sure that the actual state matches the desired one. This is how our CRD spec will 
look like: 

```go
// MyOllamaSpec defines the desired state of MyOllama
type MyOllamaSpec struct {
	// Model is the Ollama model to run
	Model string `json:"model,omitempty"`
	// SuccessPrompt is the prompt used to promote a new version
	SuccessPrompt string `json:"successPrompt,omitempty"`
}
```

You will learn more about the details of the spec in part two, but let's see now how to create the
CRD and install it in our local K8S cluster. Run the following command in the project that you created
using kubebuilder during the setup of the project.

```
kubebuilder create api --group talentarena --version v1 --kind MyOllama
# Press y to create both the controller and the resource 
```

Many files will be automatically generated by kubebuilder, but the following ones are important:

* `api/v1/myollama_types.go`: This is where the spec of the CRD will be defined.
* `internal/controller/myollama_controller.go`: This is where we will write the code of our controller
to interact with the CRD.

> ðŸ› ï¸ Now it is your turn to update the spec of the CRD. You can do that by editing the `MyOllamaSpec`
> struct from `api/v1/myollama_types.go` with the fields defined above. 

We are now ready to deploy our CRD to the K8S cluster. 

## Deploying a CRD to the K8S cluster

Kubebuilder provides a Makefile to help us installing the CRD to the cluster. Run the following commands

```
make manifests
make install
```

Now the CRD has been installed to your cluster. Run the following command to confirm that the CRD is ready.

```
kubectl get crds
```

The output should look like this:

```
NAME                        CREATED AT
myollamas.ta.talent.arena   2025-03-05T13:00:00Z
```

Since the CRD is installed in the cluster it means that we are ready to create resources of this new 
type. Let's do that!

Kubebuilder helps us to create new resources of our CRD type by creating a sample yaml file based 
on the fields we defined in the spec. Feel free to change the sample file and run the command
below once you are ready: 

```
kubectl create -f config/samples/ta_v1_myollama.yaml
```

This will create a new resource of the `myollamas.ta.talent.arena` type. Confirm that the resource
has been created successfully by running this command: 

```
kubectl get myollama -o yaml
```

The output should look like this: 

```
apiVersion: v1
items:
- apiVersion: ta.talent.arena/v1
  kind: MyOllama
  metadata:
    creationTimestamp: "2025-03-05T13:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: kustomize
      app.kubernetes.io/name: talent-v2
    name: myollama-sample
    namespace: default
    resourceVersion: "545"
    uid: 63ff3d5a-705f-4ec5-bab4-761521badf6a
  spec:
    model: llm model
    successPrompt: the prompt
kind: List
metadata:
  resourceVersion: ""
```

You have just created an instance of your custom resource! You can edit it by running the following
command: 

```
kubectl edit myollama myollama-sample
```

Nothing has happened after editing the resource since the controller is still not deployed in the cluster.
Let's see how to do this in the following section.

## Deploy the K8S controller

Kubebuilder also helps us to run the controller in our cluster by providing a Makefile command. 
You can run the controller by running `make run`. You should see the following logs:

```
2025-02-23T20:48:06+01:00	INFO	setup	starting manager
2025-02-23T20:48:06+01:00	INFO	starting server	{"name": "health probe", "addr": "[::]:8081"}
2025-02-23T20:48:06+01:00	INFO	Starting EventSource	{"controller": "myollama", "controllerGroup": "ta.talent.arena", "controllerKind": "MyOllama", "source": "kind source: *v1.MyOllama"}
2025-02-23T20:48:06+01:00	INFO	Starting Controller	{"controller": "myollama", "controllerGroup": "ta.talent.arena", "controllerKind": "MyOllama"}
2025-02-23T20:48:06+01:00	INFO	Starting workers	{"controller": "myollama", "controllerGroup": "ta.talent.arena", "controllerKind": "MyOllama", "worker count": 1}
```

This output confirms that our controller is running in the K8S cluster. Since we have not modified 
its code, the controller is still not able to react to changes on the CRD spec. Let's see how 
to do that. 

You will need to edit the `Reconcile` method in `internal/controller/myollama_controller.go`.
Add a log message like the one below:

```
logger := log.FromContext(ctx)
logger.Info("reacting to a CRD change")
```

Now try to delete the custom resource you created before. You can do that by executing: 

```
kubectl delete myollamas.ta.talent.arena myollama-sample
```

You should see the log you added before: 

```
2025-02-23T20:59:09+01:00	INFO	reacting to a CRD change	{"controller": "myollama", "controllerGroup": "ta.talent.arena", "controllerKind": "MyOllama", "MyOllama": {"name":"myollama-sample","namespace":"default"}, "namespace": "default", "name": "myollama-sample", "reconcileID": "48e1cff7-5154-4ef1-8fde-e7fb082b9169"}
```

Reacting with a hardcoded log message is not really useful, since the controller will need to know
which is the desired state defined by whoever created/changed the CRD by checking the spec fields.
To do that you will need to change the Reconcile loop. You can use the code below as a reference, 
but feel free to do it by yourself.

```go
	logger := log.FromContext(ctx)

	var ollama talentarenav1.MyOllama
	if err := r.Get(ctx, req.NamespacedName, &ollama); err != nil {
		if errors.IsNotFound(err) {
			// Resource not found. It might have been deleted after the reconcile request.
			return ctrl.Result{}, nil
		}
		return ctrl.Result{}, err
	}

	model := ollama.Spec.Model
	successPrompt := ollama.Spec.SuccessPrompt

	logger.Info("The CRD changed!", "Model", model, "SuccessPrompt", successPrompt)

	return ctrl.Result{}, nil

```

Now, whenever you create a new resource you should see something like this: 

```
2025-02-25T12:53:09+01:00	INFO	The CRD changed!	{"controller": "myollama", "controllerGroup": "talentarena", "controllerKind": "MyOllama", "MyOllama": {"name":"myollamadefault"}, "namespace": "default", "name": "myollama-sample", "reconcileID": "431e07fc-90d4-4a49-a227-9e2e8ba3aeda", "Model": "llm model", "SuccessPrompt": "the prompt"}
```

You can see that the `Model` and `SuccessPrompt` fields include the Spec fields you defined.

Congratulations!! You have created a new CRD and deployed a controller that is able to react
to changes of your CRD resources ðŸŽ‰

You are ready to start part two, where you will focus on implementing the required controller logic
to automatically deploy new LLM models based on a canary condition. 
